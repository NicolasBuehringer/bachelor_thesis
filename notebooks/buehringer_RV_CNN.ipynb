{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c88301-57af-49bb-8284-ef5258816c6e",
   "metadata": {
    "id": "b5c88301-57af-49bb-8284-ef5258816c6e"
   },
   "outputs": [],
   "source": [
    "# preprocess imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# data types\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "# CNN imports\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b667cb30-cd32-48de-885a-404db6e933ca",
   "metadata": {},
   "source": [
    "# READ ME\n",
    "\n",
    "1. if running on google colab, mount driver with this code\n",
    "and adjust path accordingly\n",
    "\n",
    "2. Master function that runs whole project is at the bottom before optional plotting functions\n",
    "\n",
    "3. The whole project can be tested by just running all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e434d8a-f03f-40af-a786-aba22411b285",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex9-I4onK82I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ex9-I4onK82I",
    "outputId": "d1c2aeea-ff4e-4c0a-df40-0c541b12fc3a"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467af6b7-bca4-4598-aec1-e9b5c6b6a8f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "467af6b7-bca4-4598-aec1-e9b5c6b6a8f3",
    "outputId": "1433ecfa-d41e-4881-9967-d6eff79183fb"
   },
   "outputs": [],
   "source": [
    "def load_raw_data_dic(folder_path: str) -> Dict:\n",
    "    '''\n",
    "    Load CSV files from a specified folder into a dictionary of DataFrames.\n",
    "    '''\n",
    "    \n",
    "    # Use glob to get all CSV files in the folder\n",
    "    csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "    \n",
    "    # Initialize an empty dictionary to store DataFrames\n",
    "    dataframes_dic = {}\n",
    "    \n",
    "    # Loop through the list of CSV files and read each one into a DataFrame\n",
    "    for file in csv_files:\n",
    "        # Extract the file name without the folder path and extension\n",
    "        file_name = os.path.basename(file).replace('.csv', '')\n",
    "    \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file)\n",
    "    \n",
    "        # Store the DataFrame in the dictionary\n",
    "        dataframes_dic[file_name] = df\n",
    "    \n",
    "    # Optionally, display the keys of the dictionary to see the loaded DataFrames\n",
    "    print(dataframes_dic.keys())\n",
    "\n",
    "    return dataframes_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bddef0-376d-4d00-ab2e-2b8bb1452672",
   "metadata": {},
   "source": [
    "# Preprocess and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d15c1-4552-4f6c-809a-becccd3d24f1",
   "metadata": {
    "id": "ed0d15c1-4552-4f6c-809a-becccd3d24f1"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataframe(data: pd.DataFrame, split_size: float = 0.85) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    Preprocess the input DataFrame by calculating log returns and realized volatility, \n",
    "    and split it into training and testing sets.\n",
    "    '''\n",
    "\n",
    "    df = data.copy()\n",
    "    df[\"Date Time\"] = pd.to_datetime(df[\"Date Time\"])\n",
    "\n",
    "    # calculate log returns\n",
    "    df[\"Log_Returns\"] = np.log(df[\"Close\"] / df[\"Close\"].shift(1))\n",
    "\n",
    "    # remove every first minute of each day\n",
    "    df = df[~(df['Date Time'].dt.time == pd.Timestamp('09:35').time())].reset_index()\n",
    "    \n",
    "    # Create a flag to group by days (380 rows)\n",
    "    df[\"day\"] = (df.index // 380)\n",
    "\n",
    "    # Calculate the RV for each grouped 380 row day\n",
    "    group_sums = df.groupby(\"day\")[\"Log_Returns\"].transform(lambda x: np.sqrt(np.sum(x**2)))\n",
    "    \n",
    "    # Add the group sums as a new column\n",
    "    df[\"realized_vol\"] = group_sums\n",
    "    \n",
    "    df.set_index(\"Date Time\", inplace=True)\n",
    "\n",
    "    # calculate train test split; make sure no day is cut in half between the two data splits\n",
    "    split_index = int(np.floor(df.shape[0] * split_size / 380) * 380)\n",
    "\n",
    "    # Split the DataFrame into training and testing sets\n",
    "    df_train = df.iloc[:split_index]\n",
    "    df_test = df.iloc[split_index:]\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbefd4c5-af74-4cb0-ac65-d656da30b1f0",
   "metadata": {
    "id": "dbefd4c5-af74-4cb0-ac65-d656da30b1f0"
   },
   "outputs": [],
   "source": [
    "def create_subsequences(dataframe: pd.DataFrame) -> Tuple[List[pd.DataFrame], List[float]]:\n",
    "    '''\n",
    "    Create subsequences for training and a list of RV targets\n",
    "    '''\n",
    "\n",
    "    # create lists to fill with subsequences and their Realized Volatility targets\n",
    "    sequence_list = []\n",
    "    sequence_target = []\n",
    "\n",
    "    # Loop through the DataFrame to create subsequences\n",
    "    for i in range(int(len(dataframe)/380-20)):\n",
    "        \n",
    "        # Extract a subsequence of 21 days (each day with 380 rows)\n",
    "        tmp_subsequence = dataframe.iloc[i * 380: (21 + i) * 380]\n",
    "        sequence_list.append(tmp_subsequence)\n",
    "\n",
    "        # Try to get the target value for the current subsequence\n",
    "        try:\n",
    "            subsequence_target = dataframe.iloc[(21 + i) * 380][\"realized_vol\"]\n",
    "            sequence_target.append(subsequence_target)\n",
    "        except:\n",
    "            # Print a message if an IndexError occurs (likely in the last iteration)\n",
    "            # since the last 21 day window doesn't have a RV target\n",
    "            print(\"last iteration\")\n",
    "\n",
    "    return sequence_list, sequence_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a5a12-d974-40d7-8c7a-2a509a4ee912",
   "metadata": {
    "id": "f06a5a12-d974-40d7-8c7a-2a509a4ee912"
   },
   "outputs": [],
   "source": [
    "def one_month_to_image(one_month: np.ndarray, add_blue: bool = False) -> np.ndarray:\n",
    "    '''\n",
    "    Convert a month's worth of log returns data into image representation of size 21x380x2\n",
    "    If images are needed for plotting, add blue channel and return 21x380x3 images\n",
    "    '''\n",
    "    \n",
    "    size = len(one_month)\n",
    "\n",
    "    # Initialize arrays for the RGB channels; should be (7980,)\n",
    "    red = np.zeros(size)\n",
    "    green = np.zeros(size)\n",
    "    blue = np.zeros(size)\n",
    "\n",
    "    # adding negative returns absolute values to red channel\n",
    "    # and positive returns to green channel\n",
    "    # fill other channel with zero\n",
    "    \n",
    "    # leave blue channel at zeros\n",
    "    for i in range(len(one_month)):\n",
    "        log_return = one_month[i]\n",
    "\n",
    "        if log_return < 0:\n",
    "            red[i] = abs(log_return)\n",
    "        elif log_return > 0:\n",
    "            green[i] = log_return\n",
    "        elif log_return == 0:\n",
    "            continue\n",
    "\n",
    "    # turning shape (7980,) into (7980,1) for scaling\n",
    "    red = np.reshape(np.array(red), (-1,1))\n",
    "    green = np.reshape(np.array(green), (-1,1))\n",
    "    \n",
    "    # scaling returns to [0, 255] interval\n",
    "    mm_scaler_red = MinMaxScaler(feature_range=(0,255))\n",
    "    red_scaled = mm_scaler_red.fit_transform(red)\n",
    "\n",
    "    mm_scaler_green = MinMaxScaler(feature_range=(0,255))\n",
    "    green_scaled = mm_scaler_green.fit_transform(green)\n",
    "\n",
    "    # Flatten the scaled arrays back to (size,) to stack\n",
    "    red_scaled_flat = red_scaled.flatten()\n",
    "    green_scaled_flat = green_scaled.flatten()\n",
    "\n",
    "    # flag if blue color channel is needed for plotting\n",
    "    if add_blue:\n",
    "        # Stack the red, green, and blue channels into a single array\n",
    "        flat_image = np.column_stack((red_scaled_flat,\n",
    "                                      green_scaled_flat,\n",
    "                                      blue))\n",
    "         # Reshape the array to (21, 380, 3) for image representation\n",
    "        square_image = flat_image.reshape((21, 380, 3))\n",
    "\n",
    "    # don't add blue channel if images are used in CNN\n",
    "    else:\n",
    "        # Stack the red and green channels into a single array\n",
    "        flat_image = np.column_stack((red_scaled_flat,\n",
    "                                      green_scaled_flat))#,\n",
    "                                      #blue))\n",
    "    \n",
    "        # Reshape the array to (21, 380, 2) for CNN training\n",
    "        square_image = flat_image.reshape((21, 380, 2))\n",
    "\n",
    "    return square_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ea34d-c366-4d26-ad8c-a6f6763d75bd",
   "metadata": {
    "id": "c62ea34d-c366-4d26-ad8c-a6f6763d75bd"
   },
   "outputs": [],
   "source": [
    "def create_images(sequence_list: List[pd.DataFrame]) -> List[np.ndarray]:\n",
    "    '''\n",
    "    Convert a list of DataFrame subsequences each containin 21 days\n",
    "    into a list of their image representations\n",
    "    '''\n",
    "\n",
    "    image_list = []\n",
    "\n",
    "    # Loop through each rolling month DataFrame in the sequence list\n",
    "    for rolling_month in sequence_list:\n",
    "        \n",
    "        # Extract the 'Log_Returns' column as a numpy array\n",
    "        X_array = np.array(rolling_month[\"Log_Returns\"])\n",
    "\n",
    "        # Convert each month into its image representation\n",
    "        image = one_month_to_image(X_array)\n",
    "\n",
    "        image_list.append(image)\n",
    "\n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a91954-beb4-4ce2-a79b-91b039df70f4",
   "metadata": {
    "id": "75a91954-beb4-4ce2-a79b-91b039df70f4"
   },
   "outputs": [],
   "source": [
    "def get_subsequence_images(dataframe: pd.DataFrame) -> Tuple[List[np.ndarray], np.ndarray]:\n",
    "    '''\n",
    "    Helper function: Generate image representations for subsequences of log returns from the input DataFrame.\n",
    "    '''\n",
    "\n",
    "    # Create subsequences and their RV targets from the input DataFrame\n",
    "    month_sequences, subsequence_targets = create_subsequences(dataframe)\n",
    "\n",
    "     # Generate images for the created subsequences\n",
    "    image_list = create_images(month_sequences)\n",
    "\n",
    "    # Convert the list of target values to a numpy array and return the images and their targets\n",
    "    return image_list, np.array(subsequence_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ccab0a-d005-47f3-8d5e-0d246e6e0960",
   "metadata": {
    "id": "c0ccab0a-d005-47f3-8d5e-0d246e6e0960"
   },
   "outputs": [],
   "source": [
    "def initialize_model() -> Sequential:\n",
    "    '''\n",
    "    Initialize and return a Sequential convolutional neural network model.\n",
    "    \n",
    "    The model architecture consists of:\n",
    "    - Input layer with shape (21, 380, 2)\n",
    "    - Convolutional layer with 16 filters, kernel size (3, 5), 'relu' activation, and 'same' padding\n",
    "    - MaxPooling2D layer with pool size (2, 2)\n",
    "    - Dropout layer with a rate of 0.2\n",
    "    - Convolutional layer with 32 filters, kernel size (3, 3), 'relu' activation, and 'same' padding\n",
    "    - Dropout layer with a rate of 0.2\n",
    "    - Flatten layer\n",
    "    - Dense layer with 1 unit and 'linear' activation\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape = (21, 380, 2)))\n",
    "    \n",
    "    model.add(Conv2D(16, (3,5), activation = \"relu\", padding = \"same\"))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu', padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1, activation = \"linear\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af437b2b-5d15-48be-bc53-88f0fdfdcf15",
   "metadata": {
    "id": "af437b2b-5d15-48be-bc53-88f0fdfdcf15"
   },
   "outputs": [],
   "source": [
    "def compile_model(model: Sequential, learning_rate: float = 0.0001) -> Sequential:\n",
    "    '''\n",
    "    Compile the given Sequential model with mean squared error loss and Adam optimizer.\n",
    "    '''\n",
    "\n",
    "    learning_rate = 0.0001  # Example value\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss = 'mse',\n",
    "                  optimizer = optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c4e05-d35b-43e3-8ee7-4baae0457090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_and_train_model(X_train: np.ndarray, y_train: np.ndarray, \n",
    "                         X_test: np.ndarray, y_test: np.ndarray) -> Tuple[List, List]:\n",
    "    '''\n",
    "    Initialize, compile, and train a convolutional neural network model on the given training data,\n",
    "    and evaluate it on the test data.\n",
    "    \n",
    "    Args:\n",
    "    X_train (np.ndarray): Training data features.\n",
    "    y_train (np.ndarray): Training data labels.\n",
    "    X_test (np.ndarray): Test data features.\n",
    "    y_test (np.ndarray): Test data labels.\n",
    "    \n",
    "    Returns:\n",
    "    Tuple[List, List]: A tuple containing two lists:\n",
    "        - First list: The trained model and the training history.\n",
    "        - Second list: The evaluation score and the test predictions.\n",
    "    '''\n",
    "\n",
    "    # Initialize the model\n",
    "    model = initialize_model()\n",
    "    # Compile the model\n",
    "    model = compile_model(model)\n",
    "    # Set up early stopping callback\n",
    "    es = EarlyStopping(patience = 10, verbose = 2)\n",
    "    # Train the model with training data\n",
    "    history = model.fit(X_train, y_train,\n",
    "                    validation_split = 0.2,\n",
    "                    callbacks = [es],\n",
    "                    epochs = 100,\n",
    "                    batch_size = 16, verbose=2)\n",
    "    \n",
    "    # Evaluate the model on test data\n",
    "    evaluate = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    # Generate predictions on the test data\n",
    "    test_predict = model.predict(X_test, verbose=0)\n",
    "\n",
    "    return [model, history], [evaluate, test_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YQRTiUBenx-w",
   "metadata": {
    "id": "YQRTiUBenx-w"
   },
   "outputs": [],
   "source": [
    "def prepare_and_train_cnn(df: pd.DataFrame) -> Tuple[List, List]:\n",
    "    '''\n",
    "    Preprocess the DataFrame, prepare training and test sets, normalize the data, \n",
    "    and train multiple CNN models.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The input DataFrame containing the returns for one stock\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List, List]: A tuple containing two lists:\n",
    "        - First list: The list of trained CNN models.\n",
    "        - Second list: The list of results including model evaluations and predictions as well as X_test and y_test.\n",
    "    '''\n",
    "\n",
    "    # Preprocess the DataFrame and split it into training and test sets\n",
    "    train, test = preprocess_dataframe(df, split_size = 0.85)\n",
    "\n",
    "    # Get subsequence images for training and test sets as well as RV targets\n",
    "    X_train, y_train = get_subsequence_images(train)\n",
    "    X_test, y_test = get_subsequence_images(test)\n",
    "\n",
    "    # Remove the last element from the training and test sets\n",
    "    # since it doesn't have a Realized Volatility target\n",
    "    del X_test[-1]\n",
    "    del X_train[-1]\n",
    "\n",
    "    # Convert the lists of subsequence images to numpy arrays\n",
    "    X_train_arr = np.array(X_train)\n",
    "    X_test_arr = np.array(X_test)\n",
    "\n",
    "    # Normalize the image data to the range [0, 1]\n",
    "    X_train_norm = X_train_arr / 255.\n",
    "    X_test_norm = X_test_arr / 255.\n",
    "\n",
    "    # Initialize and train CNN ensemble\n",
    "    cnn_0 = init_and_train_model(X_train_norm, y_train, X_test_norm, y_test)\n",
    "    print(\"Finished CNN 1\")\n",
    "    cnn_1 = init_and_train_model(X_train_norm, y_train, X_test_norm, y_test)\n",
    "    print(\"Finished CNN 2\")\n",
    "    cnn_2 = init_and_train_model(X_train_norm, y_train, X_test_norm, y_test)\n",
    "    print(\"Finished CNN 3\")\n",
    "    cnn_3 = init_and_train_model(X_train_norm, y_train, X_test_norm, y_test)\n",
    "    print(\"Finished CNN 4\")\n",
    "    cnn_4 = init_and_train_model(X_train_norm, y_train, X_test_norm, y_test)\n",
    "    print(\"Finished CNN 5\")\n",
    "\n",
    "    # Collect results and models\n",
    "    model_list = [cnn_0[0], cnn_1[0], cnn_2[0], cnn_3[0], cnn_4[0]]\n",
    "    num_results_list = [cnn_0[1], cnn_1[1], cnn_2[1], cnn_3[1], cnn_4[1], X_test_norm, y_test]\n",
    "\n",
    "    return model_list, num_results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nAtyaTwk_zcT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nAtyaTwk_zcT",
    "outputId": "4f7473a2-762b-471d-a0e1-8c228fcd7a45"
   },
   "outputs": [],
   "source": [
    "def run_and_save_model(path: str) -> None:\n",
    "    '''\n",
    "    Load raw data from the specified path, train CNN models on each DataFrame, and save the models and results.\n",
    "    dataframes_dic = load_raw_data_dic(path)\n",
    "    '''\n",
    "    counter = 1\n",
    "\n",
    "    # Load raw data from the specified path into a dictionary of DataFrames\n",
    "    dataframes_dic = load_raw_data_dic(path)\n",
    "    \n",
    "    os.makedirs('all_models', exist_ok=True)\n",
    "    os.makedirs('all_num_results', exist_ok=True)\n",
    "    \n",
    "    # Iterate over each DataFrame in the dictionary\n",
    "    for key, df in dataframes_dic.items():\n",
    "    \n",
    "        print(counter)\n",
    "        print(key)\n",
    "        # Train CNN models and get the results\n",
    "        model_dic, result_dic = prepare_and_train_cnn(df)\n",
    "\n",
    "        # Save the trained models to a file\n",
    "        with open(f'all_models/{key}.pkl', 'wb') as f:\n",
    "          pickle.dump(model_dic, f)\n",
    "\n",
    "        # Save the results to a file\n",
    "        with open(f'all_num_results/{key}.pkl', 'wb') as f:\n",
    "          pickle.dump(result_dic, f)\n",
    "    \n",
    "        counter +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dcb991-ee6d-42be-9024-d445a8c91194",
   "metadata": {},
   "source": [
    "# Master function that runs whole project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749e574-83dd-4a49-857d-27db58208946",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_and_save_model(\"Data_10_year/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cca9c-2dd8-49cc-b8b2-93edfc68a0a8",
   "metadata": {},
   "source": [
    "# Extra plotting functions\n",
    "Not cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bIssQToOQtQC",
   "metadata": {
    "id": "bIssQToOQtQC"
   },
   "outputs": [],
   "source": [
    "def plot_history(history, title='', axs=None, exp_name=\"\"):\n",
    "    if axs is not None:\n",
    "        ax1, ax2 = axs\n",
    "    else:\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    if len(exp_name) > 0 and exp_name[0] != '_':\n",
    "        exp_name = '_' + exp_name\n",
    "    ax1.plot(history.history['loss'], label = 'train' + exp_name)\n",
    "    ax1.plot(history.history['val_loss'], label = 'val' + exp_name)\n",
    "    #ax1.set_ylim(0., 0.0005)\n",
    "    ax1.set_title('loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    \"\"\"ax2.plot(history.history['mae'], label='train mae'  + exp_name)\n",
    "    ax2.plot(history.history['val_mae'], label='val mae'  + exp_name)\n",
    "    ax2.set_ylim(0, 0.35)\n",
    "    ax2.set_title('MAE')\n",
    "    ax2.legend()\"\"\"\n",
    "    return (ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z-lN2ZoOSnzS",
   "metadata": {
    "id": "Z-lN2ZoOSnzS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_pred_rv(true_rv, predict_rv):\n",
    "    # Calculate the number of subplots needed\n",
    "    num_plots = len(true_rv) // 50\n",
    "    rows, cols = 7, 2  # Setting up a 7x2 grid\n",
    "    total_plots = rows * cols\n",
    "\n",
    "    # Calculate the number of rows needed based on the actual number of plots\n",
    "    num_actual_rows = (num_plots + cols - 1) // cols  # Ceiling division\n",
    "\n",
    "    # Adjust figure size based on the number of rows needed\n",
    "    fig, axes = plt.subplots(num_actual_rows, cols, figsize=(15, num_actual_rows * 5+5))\n",
    "\n",
    "    # Flatten the axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Keep track of the current plot index, skipping segment 8\n",
    "    plot_index = 0\n",
    "\n",
    "    y_min = 0  # min(np.min(true_rv), np.min(predict_rv))\n",
    "    y_max = 0.03  # max(np.max(true_rv), np.max(predict_rv))\n",
    "\n",
    "    # Loop through the segments and create subplots\n",
    "    for i in range(num_plots):\n",
    "        # Skip segment 8 (index 7)\n",
    "        if i == 7 or i == 5:\n",
    "            continue\n",
    "        \n",
    "        start_idx = i * 50\n",
    "        end_idx = start_idx + 50\n",
    "        indices = np.arange(start_idx, end_idx)\n",
    "\n",
    "        # Determine the y-axis limits based on the data of the current segment\n",
    "        segment_true_rv = true_rv[start_idx:end_idx]\n",
    "        segment_predict_rv = predict_rv[start_idx:end_idx]\n",
    "\n",
    "        # Define the y-ticks\n",
    "        y_ticks = np.linspace(y_min, y_max, num=5)\n",
    "\n",
    "        axes[plot_index].plot(indices, segment_true_rv, marker='o', linestyle='-', color='b', label='True RV')\n",
    "        axes[plot_index].plot(indices, segment_predict_rv, marker='x', linestyle='-', color='r', label='Predict RV')\n",
    "        axes[plot_index].set_xlabel('Day in Test Set')\n",
    "        axes[plot_index].set_ylabel('Realized Volatility')\n",
    "        axes[plot_index].set_title(f'Segment {i+1}')\n",
    "        axes[plot_index].legend()\n",
    "        axes[plot_index].grid(True)\n",
    "\n",
    "        # Set the y-axis limits and ticks\n",
    "        axes[plot_index].set_ylim([y_min, y_max])\n",
    "        axes[plot_index].set_yticks(y_ticks)\n",
    "\n",
    "        plot_index += 1\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(plot_index, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0.2)  # Add space between subplots\n",
    "\n",
    "    # Save the figure with higher resolution\n",
    "    plt.savefig(\"amgen_good_results.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_pred_rv(amgen[6], amgen[3][1])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
